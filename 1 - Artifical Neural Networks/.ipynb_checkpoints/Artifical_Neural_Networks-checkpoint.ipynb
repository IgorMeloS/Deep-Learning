{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qXyD34zG_QYr"
   },
   "source": [
    "# Artificial Neural Networks in Python using Tensorflow\n",
    "\n",
    "In this program we construct an Artificial Neuron Network model. The aim is to build a classification model to predict if a certain customer will leave the bank services in the six months.\n",
    "\n",
    "**Dataset Description**\n",
    "\n",
    "For this problem we have a Dataset composed by 10000 instances (rows) and 14 features (columns). As the objective is to predict the probability of a client will leave the bank service, in our dataset the last column corresponds to the response. The others columns are the features (independent variables) that we consider to build and training the model, these columns are: RowNumber, CustomerId, Surname, CreditScore, Geography, Gender, Age, Tenure, Balance, NumOfProducts, HasCrCard, IsActiveMember, EstimatedSalary, Exited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ANWPtw_AFZu"
   },
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ushgeC5iAKCu",
    "outputId": "56b2f19f-96fa-4456-a0c0-5e8650e3fa17"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf # We utilize it to construct our ANN\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLTAs4t3AUg2"
   },
   "source": [
    "# Importing Dataset\n",
    "To train this model we  consider a data set composed by many client information (feature matrix) as  row number, customer ID, surname, credit score, geography, gender, age, tenure, balance, number of bank product, credit card, active member, estimated salary. The output is the dependent variable, that is EXIT represented by 0 to stay and 1 to leave. For this model wonâ€™t consider the three first columns (row number, customer ID, surname) because they are not relevant to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OaApzIM5AXtU",
    "outputId": "aa17c74f-49f7-4e4b-c1f4-a04460d1e6af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[619 'France' 'Female' ... 1 1 101348.88]\n",
      " [608 'Spain' 'Female' ... 0 1 112542.58]\n",
      " [502 'France' 'Female' ... 1 0 113931.57]\n",
      " ...\n",
      " [709 'France' 'Female' ... 0 1 42085.58]\n",
      " [772 'Germany' 'Male' ... 1 0 92888.52]\n",
      " [792 'France' 'Female' ... 1 0 38190.78]]\n",
      "[1 0 1 ... 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('Churn_Modelling.csv')\n",
    "X = dataset.iloc[:, 3 : -1].values # We take just the relevant features\n",
    "y = dataset.iloc[:, -1].values\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vAfvzqlxDigi"
   },
   "source": [
    "# Encoding Categorical Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lNMBBSsVDlXx",
    "outputId": "eda5f00c-7fb2-4219-fc97-b1e11d759d08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[619 'France' 0 ... 1 1 101348.88]\n",
      " [608 'Spain' 0 ... 0 1 112542.58]\n",
      " [502 'France' 0 ... 1 0 113931.57]\n",
      " ...\n",
      " [709 'France' 0 ... 0 1 42085.58]\n",
      " [772 'Germany' 1 ... 1 0 92888.52]\n",
      " [792 'France' 0 ... 1 0 38190.78]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "X[:, 2] = le.fit_transform(X[:, 2]) # Gender will be transformed to 0 (male) and 1 (female)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHsmjrNVExcN"
   },
   "source": [
    "# One Hot Econding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VPw3iUhJE2qS",
    "outputId": "ddcb325b-c92b-4154-9c13-a10ca1dfc320"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 ... 1 1 101348.88]\n",
      " [0.0 0.0 1.0 ... 0 1 112542.58]\n",
      " [1.0 0.0 0.0 ... 1 0 113931.57]\n",
      " ...\n",
      " [1.0 0.0 0.0 ... 0 1 42085.58]\n",
      " [0.0 1.0 0.0 ... 1 0 92888.52]\n",
      " [1.0 0.0 0.0 ... 1 0 38190.78]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [1])], remainder='passthrough') # Here we change the geography into dummy variables\n",
    "X = np.array(ct.fit_transform(X))\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WVPScdSdGeqx"
   },
   "source": [
    "# Splitting the Dataset into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "PKH9UuJRGkvg"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rvmtD8VQHEQ3"
   },
   "source": [
    "# Feature Scaling\n",
    "In almost ANN models we must to apply feature scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "JO1fypeSHJbV"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RoNknmZsJoBy"
   },
   "source": [
    "# Building the ANN\n",
    "To build an ANN we consider a certain number of neuron as input parameters. Trying to copy a brain, these neurons will be connected with others neurons. These neurons are hidden in a layer and the communication between the input neurons and the hidden neurons is made by an activation function. Each neuron input has a weight associated. After the first communication between the input neurons and the neurons in the first hidden layer, the communication will be between the first and the second hidden layer. And then, the second hidden layer communicates the output layer. In the output layer. This process will be repeated many times until to find the best loss function that minimize the discrepancy between the real results and the simulated results. The stochastic gradient descent is a good way to try to find a minimal in the loss function. Cross entropy is a good way to calculate the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vk4TpIBUJsRp"
   },
   "source": [
    "## Initializing the ANN\n",
    "The first step is to create an object to build a sequence layer. This sequence layer takes account the input layer (the parameters that we initialize the ANN), the hidden layers (Neurons) and output layer. To do it we utilize the modulus Tensor Flow (version 2.0 or high) that allow us to call the Keras modulus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "qLSqUQRXJ0uS"
   },
   "outputs": [],
   "source": [
    "ann = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oagB1r01eJhp"
   },
   "source": [
    "## Input Layer, first hidden layer\n",
    "In this part we create the input layer, it's means all independent variables (considered as input neurons), and then we create a first hidden layer with a number of neuron associate. When the input values go to the first hidden layer we must to choose an activation function, for this case we choose rectifier function (the weight of each input parameter is choose by the ann object). The number of neuron must be chosen by experimentation, there is no a general rule to do it. These layers are fully connected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "9BxdU1ZsfukC"
   },
   "outputs": [],
   "source": [
    "ann.add(tf.keras.layers.Dense(units=6,  activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6AZbMdTinxv"
   },
   "source": [
    "## Second hidden layer\n",
    "Here we put the second hidden layer. We can put how much we want, but for this problem we are going to consider just two hidden layers. The second hidden layer is the same to the first. But we can change it according with the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "343W94K3jFbE"
   },
   "outputs": [],
   "source": [
    "tf.keras.layers.Dropout(0.2)\n",
    "ann.add(tf.keras.layers.Dense(units=6,  activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fy_EkroCjJj0"
   },
   "source": [
    "## Output layer\n",
    "The final step is to create an output layer, We use the same object, but we make some changes in the parameters. Like this problem has a binary response yes or no, the number of neuron corresponds to 1, but if the responses gives more then two results (0, 1, 2, for exemple), we must to put the correspondent number of output as number of neuron. The second change is on the activation function, here we consider sigmoid activation function, for one simple reason, this gives us the probability of a customer leave or not the bank service. Here we constructed our artificial brain, now we must to train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9RJRX5ODjQS7"
   },
   "outputs": [],
   "source": [
    "tf.keras.layers.Dropout(0.2)\n",
    "ann.add(tf.keras.layers.Dense(units=1,  activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-v5KN4ClfZD"
   },
   "source": [
    "# Training the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sarW2BIVlkM7"
   },
   "source": [
    "## Compiling the ANN\n",
    "Compile the ANN is one of most important step. We select a method to optimize our ANN, stochastic gradient descent, represented by adam. The lost function is also very import, because from this function we are able to verify the accuracy, precision and other relevant parameters. The lost function that we are going to choose is Binary Cross Entropy (due to have a binary response). Finally the metric, we choose accuracy to verify our model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "V_kuMk2PlqWK"
   },
   "outputs": [],
   "source": [
    "ann.compile(optimizer='adam', loss='binary_crossentropy', metrics='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HceFPt3-my5m"
   },
   "source": [
    "## Training the ANN\n",
    "Now we train the ANN. Here we have two important hype parameters. Batch size determines how many results will be compared with real results. Epochs makes a certain number of repetition, in each repetition the wight of each input is changed to try a better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zn0oSK2Zm5Ig",
    "outputId": "8a4227c9-84d8-497e-d602-6f9e088cd2c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.6743 - accuracy: 0.6261\n",
      "Epoch 2/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.5074 - accuracy: 0.7974\n",
      "Epoch 3/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.4497 - accuracy: 0.8050\n",
      "Epoch 4/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.4166 - accuracy: 0.8156\n",
      "Epoch 5/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3953 - accuracy: 0.8317\n",
      "Epoch 6/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3801 - accuracy: 0.8401\n",
      "Epoch 7/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3701 - accuracy: 0.8443\n",
      "Epoch 8/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3628 - accuracy: 0.8468\n",
      "Epoch 9/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3579 - accuracy: 0.8528\n",
      "Epoch 10/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3548 - accuracy: 0.8535\n",
      "Epoch 11/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3516 - accuracy: 0.8549\n",
      "Epoch 12/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3490 - accuracy: 0.8556\n",
      "Epoch 13/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3468 - accuracy: 0.8605\n",
      "Epoch 14/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3452 - accuracy: 0.8608\n",
      "Epoch 15/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3434 - accuracy: 0.8622\n",
      "Epoch 16/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3418 - accuracy: 0.8637\n",
      "Epoch 17/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3406 - accuracy: 0.8640\n",
      "Epoch 18/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3399 - accuracy: 0.8627\n",
      "Epoch 19/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3392 - accuracy: 0.8636\n",
      "Epoch 20/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3385 - accuracy: 0.8636\n",
      "Epoch 21/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3378 - accuracy: 0.8640\n",
      "Epoch 22/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3375 - accuracy: 0.8641\n",
      "Epoch 23/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3371 - accuracy: 0.8634\n",
      "Epoch 24/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3369 - accuracy: 0.8634\n",
      "Epoch 25/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3367 - accuracy: 0.8644\n",
      "Epoch 26/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3366 - accuracy: 0.8629\n",
      "Epoch 27/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3361 - accuracy: 0.8622\n",
      "Epoch 28/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3361 - accuracy: 0.8634\n",
      "Epoch 29/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3356 - accuracy: 0.8631\n",
      "Epoch 30/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3354 - accuracy: 0.8644\n",
      "Epoch 31/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3355 - accuracy: 0.8643\n",
      "Epoch 32/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3353 - accuracy: 0.8634\n",
      "Epoch 33/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3352 - accuracy: 0.8645\n",
      "Epoch 34/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3351 - accuracy: 0.8635\n",
      "Epoch 35/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3349 - accuracy: 0.8618\n",
      "Epoch 36/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3351 - accuracy: 0.8625\n",
      "Epoch 37/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3348 - accuracy: 0.8631\n",
      "Epoch 38/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3348 - accuracy: 0.8626\n",
      "Epoch 39/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3343 - accuracy: 0.8637\n",
      "Epoch 40/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3347 - accuracy: 0.8641\n",
      "Epoch 41/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3347 - accuracy: 0.8615\n",
      "Epoch 42/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3342 - accuracy: 0.8635\n",
      "Epoch 43/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3340 - accuracy: 0.8634\n",
      "Epoch 44/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3341 - accuracy: 0.8639\n",
      "Epoch 45/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3338 - accuracy: 0.8648\n",
      "Epoch 46/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3338 - accuracy: 0.8637\n",
      "Epoch 47/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3337 - accuracy: 0.8621\n",
      "Epoch 48/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3336 - accuracy: 0.8636\n",
      "Epoch 49/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3337 - accuracy: 0.8641\n",
      "Epoch 50/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3333 - accuracy: 0.8645\n",
      "Epoch 51/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3338 - accuracy: 0.8636\n",
      "Epoch 52/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3338 - accuracy: 0.8646\n",
      "Epoch 53/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3333 - accuracy: 0.8630\n",
      "Epoch 54/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3335 - accuracy: 0.8641\n",
      "Epoch 55/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3329 - accuracy: 0.8645\n",
      "Epoch 56/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3334 - accuracy: 0.8646\n",
      "Epoch 57/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3334 - accuracy: 0.8633\n",
      "Epoch 58/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3328 - accuracy: 0.8665\n",
      "Epoch 59/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3334 - accuracy: 0.8635\n",
      "Epoch 60/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3329 - accuracy: 0.8646\n",
      "Epoch 61/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3328 - accuracy: 0.8656\n",
      "Epoch 62/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3331 - accuracy: 0.8650\n",
      "Epoch 63/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3326 - accuracy: 0.8654\n",
      "Epoch 64/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3327 - accuracy: 0.8651\n",
      "Epoch 65/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3327 - accuracy: 0.8631\n",
      "Epoch 66/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3322 - accuracy: 0.8631\n",
      "Epoch 67/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3324 - accuracy: 0.8661\n",
      "Epoch 68/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3326 - accuracy: 0.8650\n",
      "Epoch 69/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3326 - accuracy: 0.8664\n",
      "Epoch 70/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3325 - accuracy: 0.8655\n",
      "Epoch 71/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3324 - accuracy: 0.8660\n",
      "Epoch 72/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3322 - accuracy: 0.8650\n",
      "Epoch 73/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3325 - accuracy: 0.8639\n",
      "Epoch 74/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3322 - accuracy: 0.8651\n",
      "Epoch 75/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3322 - accuracy: 0.8658\n",
      "Epoch 76/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3323 - accuracy: 0.8658\n",
      "Epoch 77/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3323 - accuracy: 0.8668\n",
      "Epoch 78/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3322 - accuracy: 0.8654\n",
      "Epoch 79/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3320 - accuracy: 0.8655\n",
      "Epoch 80/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3309 - accuracy: 0.8650\n",
      "Epoch 81/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3318 - accuracy: 0.8649\n",
      "Epoch 82/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3316 - accuracy: 0.8669\n",
      "Epoch 83/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3319 - accuracy: 0.8644\n",
      "Epoch 84/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3317 - accuracy: 0.8658\n",
      "Epoch 85/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3314 - accuracy: 0.8652\n",
      "Epoch 86/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3316 - accuracy: 0.8665\n",
      "Epoch 87/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3317 - accuracy: 0.8658\n",
      "Epoch 88/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3316 - accuracy: 0.8651\n",
      "Epoch 89/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3315 - accuracy: 0.8656\n",
      "Epoch 90/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3312 - accuracy: 0.8650\n",
      "Epoch 91/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3314 - accuracy: 0.8656\n",
      "Epoch 92/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3313 - accuracy: 0.8651\n",
      "Epoch 93/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3314 - accuracy: 0.8660\n",
      "Epoch 94/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3311 - accuracy: 0.8665\n",
      "Epoch 95/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3306 - accuracy: 0.8652\n",
      "Epoch 96/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3306 - accuracy: 0.8661\n",
      "Epoch 97/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3307 - accuracy: 0.8655\n",
      "Epoch 98/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3305 - accuracy: 0.8661\n",
      "Epoch 99/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3308 - accuracy: 0.8670\n",
      "Epoch 100/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3306 - accuracy: 0.8650\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1f5801bfd0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.fit(X_train, y_train, batch_size= 32, epochs= 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqFny2LvrAbP"
   },
   "source": [
    "# Making a single prediction\n",
    "Here we make a simple prediction. Remember, Geography and Gender was changed. We must take it account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GntqBk3nrTbh",
    "outputId": "13fec3a3-eb19-40f1-95ee-2502fdebb3b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False]]\n"
     ]
    }
   ],
   "source": [
    "print(ann.predict(sc.transform([[1, 0, 0, 600, 1, 40, 3, 60000, 2, 1, 1, 50000]])) > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izaEvAa9uC0z"
   },
   "source": [
    "# Predicting the test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "543-PUCbuFsT",
    "outputId": "20862437-cd09-4512-9b14-8f62cffd3932"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]\n",
      " [0 1]\n",
      " [0 0]\n",
      " ...\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = ann.predict(X_test)\n",
    "y_pred = (y_pred > 0.5) # Here we must put it, because we have as outcome the probability, but we want a binary response.\n",
    "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVftybBMuc2J"
   },
   "source": [
    "# Making the confusion matrix and accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6vppMTWRuiEv",
    "outputId": "62ab924f-8104-4580-9044-86f1f507d8c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1522   73]\n",
      " [ 203  202]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.862"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3297 - accuracy: 0.8670\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3322 - accuracy: 0.8620\n"
     ]
    }
   ],
   "source": [
    "score_train = ann.evaluate(X_train, y_train)\n",
    "score_test = ann.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3296877145767212, 0.8669999837875366]\n"
     ]
    }
   ],
   "source": [
    "print(score_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3322369456291199, 0.8619999885559082]\n"
     ]
    }
   ],
   "source": [
    "print(score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rPoWgp3BO3R"
   },
   "source": [
    "# Conclusion\n",
    "In this program we showed a simple example of an ANN. Two important things to think are choose the number of hidden layer and the number of neuron in each hidden layer. The results are satisfactory with a good accuracy. This program can be helpfull tool for a bank in the decision about client polices."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Artifical_Neural_Networks_Igor.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
